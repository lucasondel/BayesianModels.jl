# SPDX-License-Identifier: MIT

"""
    elbo(model, X[, detailed = false, stats_scale = 1, params = params])

Compute the Evidence Lower-BOund (ELBO) of the model. If `detailed` is
set to `true` returns a tuple `elbo, loglikelihood, KL`. If
`params` is provided the function will return the natural gradient
for those parameters.
"""
elbo

function elbo(model, args...; cache = Dict(), detailed = false, stats_scale = 1)
    llh = loglikelihood(model, args..., cache)
    T = eltype(llh)
    sllh = sum(llh)*T(stats_scale)

    params = filter(isbayesianparam, getparams(model))
    KL = sum([EFD.kldiv(param.posterior, param.prior, Î¼ = param.Î¼)
              for param in params])
    detailed ? (sllh - KL, sllh, KL) : sllh - KL
end

function _diagonal(param)
    d = similar(EFD.realform(param))
    fill!(d, 1)
    Diagonal(d)
end

function âˆ‡elbo(model, cache, params)
    grads_TÎ¼ = âˆ‡sum_loglikelihood(model, cache)
    grads = Dict()
    for param in params
        Î·q = EFD.naturalform(param.posterior.param)
        Î·p = EFD.naturalform(param.prior.param)
        âˆ‚KL_âˆ‚TÎ¼ = (Î·q - Î·p)
        âˆ‚ğ“›_âˆ‚TÎ¼ = grads_TÎ¼[param] - âˆ‚KL_âˆ‚TÎ¼
        #J = EFD.jacobian(param.posterior.param)
        J = _diagonal(param.posterior.param)
        grads[param] = J * âˆ‚ğ“›_âˆ‚TÎ¼
    end
    grads
end

"""
    gradstep(param_grad; lrate)

Update the parameters' posterior by doing one natural gradient steps.
"""
function gradstep(param_grad; lrate::Real)
    for (param, âˆ‡ğ“›) in param_grad
        Î¾ = param.posterior.param.Î¾
        Î¾[:] = Î¾ + lrate*âˆ‡ğ“›
        param.Î¼.value = EFD.gradlognorm(param.posterior)
    end
end
