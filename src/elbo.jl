# Lucas Ondel, 2021

"""
    elbo(model, X[, detailed = false])

Compute the Evidence Lower-BOund (ELBO) of the model. If `detailed` is
set to `true` returns a tuple `elbo, loglikelihood, KL`.
"""
function elbo(model, args...; detailed = false, stats_scale=1)
    llh = sum(loglikelihood(model, args...))*stats_scale

    params = Zygote.@ignore filter(isbayesianparam, getparams(model))
    KL = sum(param -> EFD.kldiv(param.posterior, param.prior, Î¼ = param.Î¼),
             params)
    detailed ? (llh - KL, llh, KL) : llh - KL
end

"""
    âˆ‡elbo(model, args...[, stats_scale = 1])

Compute the natural gradient of the elbo w.r.t. to the posteriors'
parameters.
"""
function âˆ‡elbo(model, args...; params, stats_scale = 1)
    P = Params([param.Î¼ for param in params])
    ğ“›, back = Zygote.pullback(() -> elbo(model, args..., stats_scale = stats_scale), P)

    Î¼grads = back(1)

    grads = Dict()
    for param in params
        âˆ‚ğ“›_âˆ‚Î¼ = Î¼grads[param.Î¼]

        # The next two lines are equivalent to:
        #Î¾ = EFD.realform(param.posterior.param)
        #J = inv(FD.jacobian(param.posterior.param.Î¾_to_Î·, Î¾))

        #Î· = EFD.naturalform(param.posterior.param)
        #J = FD.jacobian(param.posterior.param.Î·_to_Î¾, Î·)
        J = EFD.jacobian(param.posterior.param)

        grads[param] = J * âˆ‚ğ“›_âˆ‚Î¼
    end
    ğ“›, grads
end

"""
    gradstep(param_grad; lrate)

Update the parameters' posterior by doing one natural gradient steps.
"""
function gradstep(param_grad; lrate::Real)
    for (param, âˆ‡ğ“›) in param_grad
        Î¾ = param.posterior.param.Î¾
        Î¾[:] = Î¾ + lrate*âˆ‡ğ“›
        param.Î¼[:] = EFD.gradlognorm(param.posterior)
    end
end

